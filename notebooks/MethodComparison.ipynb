{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from proj1_helpers import load_csv_data, predict_labels\n",
    "from implementations import split_data, least_square, least_squares_GD, least_squares_SGD, build_poly, build_poly_test, ridge_regression, logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def print_prediction(y, x, w):\n",
    "    y_pred = predict_labels(w, x)\n",
    "\n",
    "    good = np.sum(y_pred == y)\n",
    "    wrong = len(y_pred) - good\n",
    "\n",
    "    print(f\"Good prediction: {good}/{len(y)} ({100*good/len(y)}%)\")\n",
    "    print(f\"Wrong prediction: {wrong}/{len(y)} ({100*wrong/len(y)}%)\")\n",
    "\n",
    "    return good, wrong\n",
    "\n",
    "def prediction_log(y, x, w):\n",
    "    def sigmoid(t):\n",
    "        \"\"\"apply sigmoid function on t.\"\"\"\n",
    "        result = t\n",
    "        result[t>60] = 1\n",
    "        result[t<-60] = 0\n",
    "        result[np.abs(t) < 60] = 1/(1+np.exp(result[np.abs(t) < 60]))\n",
    "\n",
    "        return result\n",
    "    #sigmoid = lambda t: 1/(1+np.exp(-t))\n",
    "\n",
    "    y_pred = sigmoid(np.dot(x, w))\n",
    "    y_pred[y_pred <= 0.5] = 1\n",
    "    y_pred[y_pred > 0.5] = 0\n",
    "    \n",
    "    good = np.sum(y_pred == y)\n",
    "    wrong = len(y_pred)-good   \n",
    "            \n",
    "    print(f\"Good prediction: {good}/{len(y)} ({100*good/len(y)}%)\")\n",
    "    print(f\"Wrong prediction: {wrong}/{len(y)} ({100*wrong/len(y)}%)\")\n",
    "\n",
    "    return good, wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tX, ids = load_csv_data('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "x_train, y_train, x_test, y_test = split_data(tX, y, ratio, random_enabled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least squares regression using a normal equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.8237598026690033\n"
     ]
    }
   ],
   "source": [
    "loss_ls, w_ls = least_square(y_train, x_train)\n",
    "print(f\"Loss = {loss_ls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good prediction: 37093/50000 (74.186%)\n",
      "Wrong prediction: 12907/50000 (25.814%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37093, 12907)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_prediction(y=y_test, x=x_test, w=w_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iter=0, loss=6300.698795844998, diff=6300.698795844998\n",
      "  Iter=100, loss=84.23727770767559, diff=-6216.461518137323\n",
      "  Iter=200, loss=0.8057982278360653, diff=-83.43147947983952\n",
      "  Iter=211, loss=0.7582753709415647, diff=-0.04752285689450053\n",
      "Loss = 0.7582753709415647\n"
     ]
    }
   ],
   "source": [
    "max_iters = 300\n",
    "gamma = 0.1\n",
    "initial_w = np.ones(tX.shape[1])\n",
    "\n",
    "loss_lsgd, w_lsgd = least_squares_GD(y_train, x_train, initial_w, max_iters, gamma)\n",
    "print(f\"Loss = {loss_lsgd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good prediction: 31405/50000 (62.81%)\n",
      "Wrong prediction: 18595/50000 (37.19%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31405, 18595)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_prediction(y=y_test, x=x_test, w=w_lsgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iter=0, loss=6300.698795844998, diff=6300.698795844998\n",
      "  Iter=100, loss=202.69265096969772, diff=-6098.0061448753\n",
      "  Iter=200, loss=6.350545003478505, diff=-196.3421059662192\n",
      "  Iter=264, loss=0.7582753304011594, diff=-5.592269673077346\n",
      "Loss = 0.7582753304011594\n"
     ]
    }
   ],
   "source": [
    "max_iters = 300\n",
    "gamma = 0.1\n",
    "initial_w = np.ones(tX.shape[1])\n",
    "\n",
    "loss_lssgd, w_lssgd = least_squares_SGD(y_train, x_train, initial_w, max_iters, gamma)\n",
    "print(f\"Loss = {loss_lssgd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00010542, 0.00010542, 0.00010542, 0.00010542, 0.00010542,\n",
       "       0.00010542, 0.00010542, 0.00010542, 0.00010542, 0.00010542,\n",
       "       0.00010542, 0.00010542, 0.00010542, 0.00010542, 0.00010542,\n",
       "       0.00010542, 0.00010542, 0.00010542, 0.00010542, 0.00010542,\n",
       "       0.00010542, 0.00010542, 0.00010542, 0.00010542, 0.00010542,\n",
       "       0.00010542, 0.00010542, 0.00010542, 0.00010542, 0.00010542])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_lssgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good prediction: 0/50000 (0.0%)\n",
      "Wrong prediction: 50000/50000 (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jelena Banjac\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 50000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_log(y=y_test, x=x_test, w=loss_lssgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_poly = build_poly(x_train, 8, True, True,True)\n",
    "#loss_rr, w_rr =  = ridge_regression(y_train, x_train_poly, 0.0001)\n",
    "#loss_j0, w_pred_jet0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3392913481234161,\n",
       " array([ 7.95640140e-05, -7.30675186e-03, -5.98167928e-03, -4.74061106e-04,\n",
       "        -1.64878374e-02,  4.69506730e-04, -2.56830765e-02,  3.21131490e-01,\n",
       "         2.17751032e-04,  4.29827301e-03, -2.22919788e-01,  9.50531915e-02,\n",
       "         6.00933663e-02,  3.82670419e-03,  4.70605901e-04, -7.40111252e-04,\n",
       "         8.75606157e-03, -6.18134956e-04,  2.06202775e-03,  3.73446452e-03,\n",
       "        -1.52185508e-05, -5.84149473e-04, -3.25102896e-01, -1.26543580e-03,\n",
       "         7.17071976e-04,  9.92193723e-04, -1.29782335e-03, -5.36689228e-03,\n",
       "        -1.12833292e-02, -4.07460235e-03]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rr, w_rr = ridge_regression(y_train, x_train, 0.0001)\n",
    "loss_rr, w_rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good prediction: 37074/50000 (74.148%)\n",
      "Wrong prediction: 12926/50000 (25.852%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37074, 12926)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_prediction(y=y_test, x=x_test, w=w_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression using gradient descent or SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iter=0, loss=-395568887.07290137, diff=-395568887.07290137\n",
      "  Iter=100, loss=-58628137731824.266, diff=-58627742162937.195\n",
      "  Iter=200, loss=-117256014124142.34, diff=-58627876392318.08\n",
      "  Iter=300, loss=-175883890516460.5, diff=-58627876392318.16\n",
      "  Iter=400, loss=-234511766908778.66, diff=-58627876392318.16\n",
      "  Iter=500, loss=-293139644984082.25, diff=-58627878075303.59\n",
      "  Iter=600, loss=-351767525141671.2, diff=-58627880157588.94\n",
      "  Iter=700, loss=-410395405299260.0, diff=-58627880157588.81\n",
      "  Iter=800, loss=-469023285456848.56, diff=-58627880157588.56\n",
      "  Iter=900, loss=-527651165614436.7, diff=-58627880157588.125\n",
      "  Iter=1000, loss=-586279045772024.6, diff=-58627880157587.94\n",
      "  Iter=1100, loss=-644906925929612.6, diff=-58627880157588.0\n",
      "  Iter=1200, loss=-703534806087200.8, diff=-58627880157588.125\n",
      "  Iter=1300, loss=-762162686244788.8, diff=-58627880157588.0\n",
      "  Iter=1400, loss=-820790566402377.2, diff=-58627880157588.5\n",
      "  Iter=1500, loss=-879418446559966.0, diff=-58627880157588.75\n",
      "  Iter=1600, loss=-938046326717554.9, diff=-58627880157588.875\n",
      "  Iter=1700, loss=-996674206875143.4, diff=-58627880157588.5\n",
      "  Iter=1800, loss=-1055302087032731.4, diff=-58627880157588.0\n",
      "  Iter=1900, loss=-1113929967190319.0, diff=-58627880157587.625\n",
      "  Iter=1999, loss=-1171971568546331.2, diff=-58041601356012.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1171971568546331.2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iters = 2000\n",
    "gamma = 1e-5\n",
    "lamb = 1\n",
    "initial_w = np.ones(x_train.shape[1])\n",
    "\n",
    "loss_lr, w_lr = logistic_regression(y_train, x_train, initial_w, max_iters, gamma)\n",
    "loss_lr\n",
    "#w_star, min_loss = get_best_model(losses, ws)\n",
    "#loss_lr, w_lr = logistic_regression(y_train, tX_train, gamma, max_iters, False)\n",
    "#w_star, min_loss = get_best_model(losses, ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good prediction: 10661/14758 (72.238786%)\n",
      "Wrong prediction: 4097/14758 (27.761214%)\n"
     ]
    }
   ],
   "source": [
    "print_prediction(y_test, x_train, w_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
